{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/spaceship-titanic/train.csv')\ndf_test = pd.read_csv('../input/spaceship-titanic/test.csv')\ndf_sample = pd.read_csv('../input/spaceship-titanic/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rm_many_null_rows(df):\n    # df should only be on training data\n    list_count_of_null = []\n    for i in range(len(df)):\n        count_of_null = 0\n        for j in df.columns:\n#             print(f'{df[j].iloc[i]} --- ', end='')\n            if str(df[j].iloc[i]).lower() == 'nan':\n                count_of_null += 1\n#         print('')\n        list_count_of_null.append(count_of_null)\n    df['NullCount'] = list_count_of_null\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = rm_many_null_rows(df_train)\ndf_train = df_train[df_train['NullCount'] < 3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature engineering\n\ndef feat_eng(df):\n    # cabin feat group\n    cabin_group_list = []\n    for cabin in df['Cabin']:\n        try:\n            cabin_group_list.append(cabin.split('/')[0])\n        except:\n            cabin_group_list.append(np.nan)\n    df['CabinGroup'] = cabin_group_list\n    \n    # cabin feat zone\n    cabin_zone_list = []\n    for cabin in df['Cabin']:\n        try:\n            cabin_zone_list.append(cabin.split('/')[2])\n        except:\n            cabin_zone_list.append(np.nan)\n    df['CabinZone'] = cabin_zone_list\n    \n    # family size\n    passengerId_group_list = []\n    for passenger in df['PassengerId']:\n        try:\n            passengerId_group_list.append(str(passenger.split('_')[0]))\n        except:\n            passengerId_group_list.append(np.nan)       \n    df['PassengerIdGroup'] = passengerId_group_list\n    \n    df['FamilySize'] = df.groupby(['PassengerIdGroup'], dropna=False)['PassengerIdGroup'].transform('count')\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_eng(df_train).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Separate target from predictors\ny = feat_eng(df_train).Transported\nX = feat_eng(df_train).drop(['Transported'], axis=1)\n\n# Divide data into training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n# categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n#                         X_train_full[cname].dtype == \"object\"]\ncategorical_cols = ['HomePlanet', 'CryoSleep', 'CabinGroup', 'CabinZone', 'Destination', 'VIP', 'FamilySize'] # 'Name'\n\n# Select numerical columns\nnumerical_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n\n# Keep selected columns only\nmy_cols = ['PassengerId'] + categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\ndef get_score(preprocessor, model):\n    # Bundle preprocessing and modeling code in a pipeline\n    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                  ('model', model)\n                                 ])\n    \n    from sklearn.model_selection import cross_val_score\n    scores = cross_val_score(my_pipeline, X, y,\n                                  cv=5,\n                                  scoring='accuracy')\n    return scores\n# print(f\"Accuracy scores:\\n{scores}\\n\")\n# print(f\"Average scores: {round(scores.mean()*100, 2)}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\n\nmodels = []\n# models.append(RandomForestClassifier(n_estimators=125, random_state=0))\n# models.append(RandomForestClassifier(n_estimators=150, random_state=0))\n# models.append(RandomForestClassifier(n_estimators=175, random_state=0))\n# models.append(RandomForestClassifier(n_estimators=200, random_state=0))\n# models.append(RandomForestClassifier(n_estimators=1000, random_state=0))\n\n\n# models.append(XGBClassifier(random_state=0))\nmodels.append(XGBClassifier(n_estimators=100, learning_rate=0.001, random_state=0))\n# models.append(KNeighborsClassifier(n_neighbors=45))\n# models.append(MultinomialNB())\n# models.append(SVC(kernel='linear'))\n\n\nfor model in models:\n    score = get_score(preprocessor, model)\n    print(f\"{model}\\nAvg accuracy: {round(score.mean()*100, 2)}%\\nStd of accuracy: {round(score.std()*100, 2)}%\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nmodel = models[0]\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submitting\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(feat_eng(df_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'PassengerId': df_test['PassengerId'],\n                       'Transported': [bool(i) for i in preds]})\noutput.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}